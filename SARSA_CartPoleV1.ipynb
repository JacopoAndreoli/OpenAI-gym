{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mKernel Python 3.8.10 is not usable. Check the Jupyter output tab for more information. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from CartPole_env import CartPole_v1\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from mpl_toolkits import mplot3d\n",
    "import time \n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "'''\n",
    "default parameters of the environment:\n",
    "n_obs = 1000\n",
    "n_split = [4,5,10,12]\n",
    "sim = True\n",
    "PLOT_DEBUG = False\n",
    "'''\n",
    "env = CartPole_v1(n_split=[10,10,12,12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Let's resolve the same environment exploiting the SARSA algorithm, based on the TD(0) idea\n",
    "'''\n",
    "\n",
    "class SARSA_learning(CartPole_v1):\n",
    "    \n",
    "    def __init__(self, intervals, n_split, env, epsilon = 0.8, alpha = 0.5, gamma = 0.9):  # in this case, we will use alpha as constant parameter for the update\n",
    "                                                                                          # not alpha = 1/n as it is in the case of the MC implementation\n",
    "        self.intervals = intervals\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.env = env\n",
    "        self.n_split = n_split\n",
    "        self.timestamps = []\n",
    "        Q_height = 2 # since there are only two possible action \n",
    "        Q_width = (n_split[0]+2)*(n_split[1]+2)*(n_split[2]+2)*(n_split[3]+2)\n",
    "        self.Q_table = np.zeros((2, Q_width))\n",
    "        self.Counter_table = np.zeros_like(self.Q_table)\n",
    "        \n",
    "    def episode_init(self):\n",
    "        state, info = self.env.reset()\n",
    "        #print(state)\n",
    "        state = self.state_projection(state)\n",
    "        return state\n",
    "    \n",
    "    def action_choice(self, state):\n",
    "        if((np.random.randint(0,100+1)/100) > self.epsilon):\n",
    "            action = int(np.random.choice([0, 1], size=1, p=[.5, .5]))\n",
    "        else:\n",
    "            action = np.argmax(self.Q_table[:, state]) # take the max value associated to the Q_function \n",
    "        return action  \n",
    "    \n",
    "    def Q_update(self, state, action, new_state, new_action, reward):\n",
    "        #print(\"state = {}  ; action = {}  ; new_state = {}  ; new_action = {}  ; reward = {}\".format(state, action, new_state, new_action, reward))\n",
    "        self.Q_table[action][state] += self.alpha*(reward - self.Q_table[action][state] + self.gamma*self.Q_table[new_action][new_state])\n",
    "        return\n",
    "    \n",
    "    def play_an_episode(self):\n",
    "        score = 0\n",
    "        state = self.episode_init()\n",
    "        action = self.action_choice(state)\n",
    "        while(True):\n",
    "            score +=1 \n",
    "            new_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "            if terminated or truncated: \n",
    "                self.timestamps.append(score)\n",
    "                break\n",
    "            new_state = self.state_projection(new_state)\n",
    "            new_action = self.action_choice(new_state)\n",
    "            self.Q_update(state, action, new_state, new_action, reward)\n",
    "            action = new_action \n",
    "            state = new_state\n",
    "        \n",
    "    def run(self, n_episodes=5000):\n",
    "        for _ in tqdm(range(n_episodes)):\n",
    "            self.play_an_episode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8308/100000 [00:22<04:11, 364.42it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m SARSA_agent \u001b[39m=\u001b[39m SARSA_learning(intervals \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mintervals, n_split \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mn_split, env \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39menv)\n\u001b[0;32m----> 2\u001b[0m SARSA_agent\u001b[39m.\u001b[39;49mrun(n_episodes \u001b[39m=\u001b[39;49m \u001b[39m100000\u001b[39;49m)\n\u001b[1;32m      4\u001b[0m x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(\u001b[39mlen\u001b[39m(SARSA_agent\u001b[39m.\u001b[39mtimestamps))\n\u001b[1;32m      5\u001b[0m accumul \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "Cell \u001b[0;32mIn [29], line 57\u001b[0m, in \u001b[0;36mSARSA_learning.run\u001b[0;34m(self, n_episodes)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m, n_episodes\u001b[39m=\u001b[39m\u001b[39m5000\u001b[39m):\n\u001b[1;32m     56\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(n_episodes)):\n\u001b[0;32m---> 57\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mplay_an_episode()\n",
      "Cell \u001b[0;32mIn [29], line 50\u001b[0m, in \u001b[0;36mSARSA_learning.play_an_episode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m     49\u001b[0m new_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate_projection(new_state)\n\u001b[0;32m---> 50\u001b[0m new_action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maction_choice(new_state)\n\u001b[1;32m     51\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mQ_update(state, action, new_state, new_action, reward)\n\u001b[1;32m     52\u001b[0m action \u001b[39m=\u001b[39m new_action \n",
      "Cell \u001b[0;32mIn [29], line 29\u001b[0m, in \u001b[0;36mSARSA_learning.action_choice\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39maction_choice\u001b[39m(\u001b[39mself\u001b[39m, state):\n\u001b[1;32m     28\u001b[0m     \u001b[39mif\u001b[39;00m((np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m,\u001b[39m100\u001b[39m\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m/\u001b[39m\u001b[39m100\u001b[39m) \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepsilon):\n\u001b[0;32m---> 29\u001b[0m         action \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(np\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mchoice([\u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m], size\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, p\u001b[39m=\u001b[39;49m[\u001b[39m.5\u001b[39;49m, \u001b[39m.5\u001b[39;49m]))\n\u001b[1;32m     30\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     31\u001b[0m         action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mQ_table[:, state]) \u001b[39m# take the max value associated to the Q_function \u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "SARSA_agent = SARSA_learning(intervals = env.intervals, n_split = env.n_split, env = env.env)\n",
    "SARSA_agent.run(n_episodes = 100000)\n",
    "           \n",
    "x = np.arange(len(SARSA_agent.timestamps))\n",
    "accumul = 0\n",
    "mobile_avg = []\n",
    "counter = 0\n",
    "for elem in SARSA_agent.timestamps: \n",
    "    counter += 1\n",
    "    accumul = accumul + (1/counter)*(elem-accumul) \n",
    "    mobile_avg.append(accumul)\n",
    "    \n",
    "plt.plot(x,mobile_avg, label='eps = 0.95, gamma = 1')  # replace accordingly with the parameters used in the algorithm\n",
    "plt.axhline(195.0, color='gray', label='env threshold = 195.0', linestyle='--')\n",
    "plt.xlabel('n° episodes')\n",
    "plt.ylabel('avg returns') \n",
    "plt.title('learning curve')\n",
    "plt.legend()\n",
    "plt.plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CartPole_v1' object has no attribute 'reset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# let's experience with the environment  \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m observation, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mreset()\n\u001b[1;32m      3\u001b[0m curr_state \u001b[39m=\u001b[39m SARSA_agent\u001b[39m.\u001b[39mstate_projection(observation, SARSA_agent\u001b[39m.\u001b[39mintervals)\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10000\u001b[39m):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CartPole_v1' object has no attribute 'reset'"
     ]
    }
   ],
   "source": [
    "\n",
    "# let's experience with the environment  \n",
    "observation, info = env.reset()\n",
    "curr_state = SARSA_agent.state_projection(observation, SARSA_agent.intervals)\n",
    "for _ in range(10000):\n",
    "   \n",
    "   action = SARSA_agent.action_choice(curr_state) \n",
    "   new_state, reward, terminated, truncated, info = env.step(action)\n",
    "   curr_state = SARSA_agent.state_projection(new_state, SARSA_agent.intervals)\n",
    "\n",
    "   if terminated or truncated:\n",
    "      observation, info = env.reset() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
