{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from mpl_toolkits import mplot3d\n",
    "import time \n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This cell will load the environment and select the render mode for the simulation, since it is not time consuming; \n",
    "the human node rendering is intended for the real-time simulation of the system (reported at the end of the monte-carlo section)\n",
    "'''\n",
    "\n",
    "env_name = 'CartPole-v1' \n",
    "env = gym.make(env_name, render_mode='rgb_array')   # for simulation\n",
    "#env = gym.make(env_name, render_mode='human')      # for rendering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "let's run the environment as it is so as to understand which output we are aiming for\n",
    "'''\n",
    "\n",
    "# let's experience with the environment  \n",
    "observation, info = env.reset()\n",
    "for _ in range(1000):\n",
    "   action = env.action_space.sample()  # this is where you would insert your policy\n",
    "   observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "   if terminated or truncated:\n",
    "      observation, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "As it is reported on the official documentation for this environment, the observation space is composed\n",
    "of four continous time variable. In order to apply RL method such as MC it is nedeed to espress this state in a discrete world.\n",
    "This cell analyze which are the most common output coming from the environment running 2000 action in different states. \n",
    "'''\n",
    "\n",
    "obs_list = []\n",
    "observation, info = env.reset()\n",
    "obs_list.append(observation)\n",
    "\n",
    "#here we are observaing a sequence of 10000 actions took, without considering the number of episode\n",
    "for _ in range(2000):\n",
    "    action = env.action_space.sample()  # this is where you would insert your policy\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    obs_list.append(observation)\n",
    "    if terminated or truncated:\n",
    "       observation, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This cell identify the intervals in which discretize the continous time variables\n",
    "'''\n",
    "\n",
    "def intervals_split(start, finish, parts):\n",
    "    '''\n",
    "    function that, given an interval and the number of split to apply, \n",
    "    return a list of intervals equalli separated by the number of split given as input\n",
    "    '''\n",
    "    part_duration = (finish-start) / parts\n",
    "    return [start+i * part_duration for i in range(parts+1)]\n",
    "\n",
    "# y coordinate for plotting stuff\n",
    "y = [0,1,2,3]\n",
    "\n",
    "states = [[], [], [], []]\n",
    "for k in range(len(obs_list)):\n",
    "    plt.plot(obs_list[k][0], y[0], 'o', color='gray', alpha = 0.05)\n",
    "    plt.plot(obs_list[k][1], y[1], 'o', color='gray', alpha = 0.05)\n",
    "    plt.plot(obs_list[k][2], y[2], 'o', color='gray', alpha = 0.05)\n",
    "    plt.plot(obs_list[k][3], y[3], 'o', color='gray', alpha = 0.05)\n",
    "    states[0].append(obs_list[k][0])\n",
    "    states[1].append(obs_list[k][1])\n",
    "    states[2].append(obs_list[k][2])\n",
    "    states[3].append(obs_list[k][3])\n",
    "    \n",
    "# took the extrema from the simulations \n",
    "extrema = []\n",
    "intervals = []\n",
    "'''\n",
    "This is an important parameter to set: increasing the number of split we are more accurate into discretize the \n",
    "continous state space associate to the observations. However, this will negatively have an impact on the number \n",
    "of episode needed by the algorithm to learn a correct policy\n",
    "'''\n",
    "n_split = [4,5,10,12]  # with 3 splits there are 5 intervals created, since two are that one that deal with +- infty\n",
    "\n",
    "for k in range(len(states)):\n",
    "    extrema.append([np.min(states[k]), np.max(states[k])])\n",
    "    intervals.append(intervals_split(extrema[k][0], extrema[k][1], n_split[k]))\n",
    "\n",
    "for k in range(len(states)):\n",
    "    for i in range(len(intervals[k])):\n",
    "        plt.plot(intervals[k][i], y[k], '|', color = 'red', markersize=5)\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_projection(value, intervals):\n",
    "    '''\n",
    "    This function associate to each state observation a unique positive integer value, useful\n",
    "    for constructing the Q_table\n",
    "    '''\n",
    "    #print(value)\n",
    "    #print(len(value))\n",
    "    discrete_state = []\n",
    "    n_digits = []\n",
    "    for k in range(len(value)):\n",
    "        discrete_state.append(np.digitize(value[k], intervals[k])) # the k-th state description with the k-th intervals split\n",
    "        n_digits.append(len(str(np.digitize(value[k], intervals[k]))))\n",
    "    discrete_state = discrete_state[0]+discrete_state[1]*(n_split[0]+2)+discrete_state[2]*(n_split[0]+2)*(n_split[1]+2)+discrete_state[3]*(n_split[0]+2)*(n_split[1]+2)*(n_split[2]+2)\n",
    "    \n",
    "    return discrete_state\n",
    "\n",
    "plot_timestamps = []  # this variables is used in order to make fast plot comparison changing the parameters in the initialization of\n",
    "                      # the MC_learning class in the next cell\n",
    "\n",
    "class MC_learning():\n",
    "    \n",
    "    '''\n",
    "    class that construct the Montecarlo approach to the CartPole problem. The methods inside the class \n",
    "    already have a sel-explanatory name of their functionality \n",
    "    '''\n",
    "    def __init__(self, intervals, n_split, env, epsilon = 0.98, gamma = 0.9, alpha = 0.8):\n",
    "        \n",
    "        self.intervals = intervals\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha \n",
    "        self.env = env\n",
    "        self.timestamps = []\n",
    "        Q_height = 2 # since there are only two possible action \n",
    "        Q_width = (n_split[0]+2)*(n_split[1]+2)*(n_split[2]+2)*(n_split[3]+2)\n",
    "        self.Q_table = np.array([np.random.choice([0, 1], size=Q_width, p=[.5, .5]), np.random.choice([0, 1], size=Q_width, p=[.5, .5])])        \n",
    "        self.Counter_table = np.zeros_like(self.Q_table) \n",
    "        # the counter table is used since the update of the Q_table will have a NOT constant 'alpha' that is alpha = 1/n.\n",
    "        # However, it is not always the best choice using a value that depends on the number of observed returns; it is not the case \n",
    "        # since the distribution that governing the physic process does not change over time \n",
    "        \n",
    "    def action_choice(self, state):\n",
    "        \n",
    "        rand_value = np.random.randint(0, 100+1)/100 \n",
    "        if(rand_value > self.epsilon):\n",
    "            action = int(np.random.choice([0, 1], size=1, p=[.5, .5]))\n",
    "        else:\n",
    "            action = np.argmax(self.Q_table[:, state]) \n",
    "        return action  \n",
    "    \n",
    "    def play_an_episode(self):\n",
    "        \n",
    "        obs_act_rew = []\n",
    "        curr_state, info = self.env.reset()\n",
    "        curr_state = state_projection(curr_state, self.intervals)\n",
    "        index = 0\n",
    "        episode_reward = []\n",
    "        while(True):\n",
    "            index +=1\n",
    "            action = self.action_choice(curr_state)\n",
    "            new_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "            episode_reward.append(reward)\n",
    "            obs_act_rew.append([curr_state, action])\n",
    "            curr_state = state_projection(new_state, self.intervals)\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                #print(\"episode last for {} timestemps\".format(index))\n",
    "                self.timestamps.append(index)\n",
    "                for k in range(0,len(episode_reward),1):\n",
    "                    if k == 0:\n",
    "                        returns = episode_reward\n",
    "                    else:\n",
    "                        returns = list(reversed(episode_reward))[:-k]\n",
    "                    obs_act_rew[k].append(math.fsum(returns))\n",
    "                break\n",
    "        \n",
    "        return obs_act_rew    \n",
    "\n",
    "    def update_Q_table(self, list_of_returns):\n",
    "        \n",
    "        G = 0\n",
    "        for elem in list_of_returns[::-1]:\n",
    "            state, action, reward = elem[0], elem[1], elem[2] \n",
    "            G = self.gamma*G + reward\n",
    "            self.Counter_table[action][state] += 1\n",
    "            self.Q_table[action][state] = self.Q_table[action][state] + (1/self.Counter_table[action][state])*(G-self.Q_table[action][state])\n",
    "            #self.alpha*(G-self.Q_table[action][state])\n",
    "            \n",
    "            \n",
    "    def run(self, n_episodes = 5000): \n",
    "        for _ in tqdm(range(n_episodes)):\n",
    "            list_of_returns = self.play_an_episode()\n",
    "            self.update_Q_table(list_of_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "initialization and run of the algorithm; here it is important to approapriately set the epsilon and gamma parameter \n",
    "so as to obtain the best efficiency, namely learn the optimal policy in the fastest way\n",
    "'''\n",
    "MC_agent = MC_learning(intervals = intervals, n_split = n_split, env = env, epsilon=.95, gamma=1, alpha = 0.99)\n",
    "MC_agent.run(5000)\n",
    "x = np.arange(len(MC_agent.timestamps))\n",
    "accumul = 0\n",
    "mobile_avg = []\n",
    "counter = 0\n",
    "for elem in MC_agent.timestamps: \n",
    "    counter += 1\n",
    "    accumul = accumul + (1/counter)*(elem-accumul) \n",
    "    mobile_avg.append(accumul)\n",
    "    \n",
    "plt.plot(x,mobile_avg, label='eps = 0.95, gamma = 1')  # replace accordingly with the parameters used in the algorithm\n",
    "plt.axhline(195.0, color='gray', label='env threshold = 195.0', linestyle='--')\n",
    "plt.xlabel('n° episodes')\n",
    "plt.ylabel('avg returns') \n",
    "plt.title('learning curve')\n",
    "plt.legend()\n",
    "plt.plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# cell used for plotting purpose \n",
    "# '''\n",
    "\n",
    "# overall_timestamps = []\n",
    "\n",
    "# for k in range(len(plot_timestamps)):\n",
    "#     timestamps = plot_timestamps[k]\n",
    "#     accumul = 0\n",
    "#     mobile_avg = []\n",
    "#     counter = 0\n",
    "#     for elem in timestamps: \n",
    "#         counter += 1\n",
    "#         accumul = accumul + (1/counter)*(elem-accumul) \n",
    "#         mobile_avg.append(accumul)\n",
    "#     overall_timestamps.append(mobile_avg)\n",
    "\n",
    "# x = np.arange(len(overall_timestamps[0]))\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18,8))\n",
    "# fig.suptitle('plot against epsilon and gamma parameter')\n",
    "# ax1.set_title(\"variation with different epsilon\")\n",
    "# ax1.plot(x,overall_timestamps[0], label='eps = 0.95, gamma = 1')\n",
    "# ax1.plot(x,overall_timestamps[1], label='eps = 0.90, gamma = 1')\n",
    "# ax1.plot(x,overall_timestamps[2], label='eps = 0.85, gamma = 1')\n",
    "# ax1.plot(x,overall_timestamps[3], label='eps = 0.80, gamma = 1')\n",
    "# ax1.plot(x,overall_timestamps[4], label='eps = 0.75, gamma = 1')\n",
    "# ax1.plot(x,overall_timestamps[10], label='eps = 1.00, gamma = 1')\n",
    "# ax1.axhline(195.0, color='gray', label='env threshold = 195.0', linestyle='--')\n",
    "# ax1.legend()\n",
    "\n",
    "# ax2.set_title(\"variation with different gamma\")\n",
    "# ax2.plot(x,overall_timestamps[5], label='eps = 0.90, gamma = 0.95')\n",
    "# ax2.plot(x,overall_timestamps[6], label='eps = 0.90, gamma = 0.90')\n",
    "# ax2.plot(x,overall_timestamps[7], label='eps = 0.90, gamma = 0.85')\n",
    "# ax2.plot(x,overall_timestamps[8], label='eps = 0.90, gamma = 0.80')\n",
    "# ax2.plot(x,overall_timestamps[9], label='eps = 0.90, gamma = 0.75')\n",
    "# ax2.plot(x,overall_timestamps[11], label='eps = 0.90, gamma = 1')\n",
    "# ax2.axhline(195.0, color='gray', label='env threshold = 195.0', linestyle='--')\n",
    "# ax2.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In this cell, using the correct render option, it is possible to visualize thorugh real-time simulation the\n",
    "final result associated to the policy learnt by the agent\n",
    "'''\n",
    "#env = gym.make(env_name, render_mode='rgb_array')   # for simulation\n",
    "env = gym.make(env_name, render_mode='human')       # for rendering\n",
    "\n",
    "# let's experience with the environment  \n",
    "observation, info = env.reset()\n",
    "curr_state = state_projection(observation, MC_agent.intervals)\n",
    "for _ in range(10000):\n",
    "   action = MC_agent.action_choice(curr_state) \n",
    "   new_state, reward, terminated, truncated, info = env.step(action)\n",
    "   curr_state = state_projection(new_state, MC_agent.intervals)\n",
    "\n",
    "   if terminated or truncated:\n",
    "      observation, info = env.reset() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Let's resolve the same environment exploiting the SARSA algorithm, based on the TD(0) idea\n",
    "'''\n",
    "\n",
    "class SARSA_learning():\n",
    "    \n",
    "    def __init__(self, intervals, n_split, env, epsilon = 0.98, alpha = 0.1, gamma = 0.98):  # in this case, we will use alpha as constant parameter for the update\n",
    "                                                                                          # not alpha = 1/n as it is in the case of the MC implementation\n",
    "        self.intervals = intervals\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.env = env\n",
    "        self.timestamps = []\n",
    "        Q_height = 2 # since there are only two possible action \n",
    "        Q_width = (n_split[0]+2)*(n_split[1]+2)*(n_split[2]+2)*(n_split[3]+2)\n",
    "        self.Q_table = np.zeros((2, Q_width))\n",
    "        self.Counter_table = np.zeros_like(self.Q_table)\n",
    "        \n",
    "    def episode_init(self):\n",
    "        state, info = self.env.reset()\n",
    "        #print(state)\n",
    "        state = state_projection(state, self.intervals)\n",
    "        return state\n",
    "    \n",
    "    def action_choice(self, state):\n",
    "        if((np.random.randint(0,100+1)/100) > self.epsilon):\n",
    "            action = int(np.random.choice([0, 1], size=1, p=[.5, .5]))\n",
    "        else:\n",
    "            action = np.argmax(self.Q_table[:, state]) # take the max value associated to the Q_function \n",
    "        return action  \n",
    "    \n",
    "    def Q_update(self, state, action, new_state, new_action, reward):\n",
    "        #print(\"state = {}  ; action = {}  ; new_state = {}  ; new_action = {}  ; reward = {}\".format(state, action, new_state, new_action, reward))\n",
    "        self.Q_table[action][state] += self.alpha*(reward - self.Q_table[action][state] + self.gamma*self.Q_table[new_action][new_state])\n",
    "        return\n",
    "    \n",
    "    def play_an_episode(self):\n",
    "        score = 0\n",
    "        state = self.episode_init()\n",
    "        action = self.action_choice(state)\n",
    "        while(True):\n",
    "            score +=1 \n",
    "            new_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "            \n",
    "            if terminated or truncated: \n",
    "                self.timestamps.append(score)\n",
    "                break\n",
    "            new_state = state_projection(new_state, self.intervals)\n",
    "            new_action = self.action_choice(new_state)\n",
    "            self.Q_update(state, action, new_state, new_action, reward)\n",
    "            action = new_action \n",
    "            state = new_state\n",
    "        \n",
    "    def run(self, n_episodes=5000):\n",
    "        for _ in range(n_episodes):\n",
    "            self.play_an_episode()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfor\u001b[39;00m alpha \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39;49m(\u001b[39m0.05\u001b[39;49m,\u001b[39m1.05\u001b[39;49m,\u001b[39m0.05\u001b[39;49m):\n\u001b[1;32m      2\u001b[0m     \u001b[39mprint\u001b[39m(alpha)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [1:06:27<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:75\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 75\u001b[0m     reduction \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(obj, method)\n\u001b[1;32m     76\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'prod'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [34], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m epsilon \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m: \n\u001b[1;32m     11\u001b[0m     SARSA_agent \u001b[39m=\u001b[39m SARSA_learning(intervals \u001b[39m=\u001b[39m intervals, n_split \u001b[39m=\u001b[39m n_split, env \u001b[39m=\u001b[39m env, epsilon\u001b[39m=\u001b[39mepsilon, alpha \u001b[39m=\u001b[39m alpha, gamma\u001b[39m=\u001b[39mgamma)\n\u001b[0;32m---> 12\u001b[0m     SARSA_agent\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m     13\u001b[0m     x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(\u001b[39mlen\u001b[39m(SARSA_agent\u001b[39m.\u001b[39mtimestamps))\n\u001b[1;32m     14\u001b[0m     accumul \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "Cell \u001b[0;32mIn [31], line 57\u001b[0m, in \u001b[0;36mSARSA_learning.run\u001b[0;34m(self, n_episodes)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m, n_episodes\u001b[39m=\u001b[39m\u001b[39m5000\u001b[39m):\n\u001b[1;32m     56\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_episodes):\n\u001b[0;32m---> 57\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mplay_an_episode()\n",
      "Cell \u001b[0;32mIn [31], line 50\u001b[0m, in \u001b[0;36mSARSA_learning.play_an_episode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m     49\u001b[0m new_state \u001b[39m=\u001b[39m state_projection(new_state, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintervals)\n\u001b[0;32m---> 50\u001b[0m new_action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maction_choice(new_state)\n\u001b[1;32m     51\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mQ_update(state, action, new_state, new_action, reward)\n\u001b[1;32m     52\u001b[0m action \u001b[39m=\u001b[39m new_action \n",
      "Cell \u001b[0;32mIn [31], line 28\u001b[0m, in \u001b[0;36mSARSA_learning.action_choice\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39maction_choice\u001b[39m(\u001b[39mself\u001b[39m, state):\n\u001b[1;32m     27\u001b[0m     \u001b[39mif\u001b[39;00m((np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m,\u001b[39m100\u001b[39m\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m/\u001b[39m\u001b[39m100\u001b[39m) \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepsilon):\n\u001b[0;32m---> 28\u001b[0m         action \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(np\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mchoice([\u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m], size\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, p\u001b[39m=\u001b[39;49m[\u001b[39m.5\u001b[39;49m, \u001b[39m.5\u001b[39;49m]))\n\u001b[1;32m     29\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     30\u001b[0m         action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mQ_table[:, state]) \u001b[39m# take the max value associated to the Q_function \u001b[39;00m\n",
      "File \u001b[0;32mmtrand.pyx:946\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mprod\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3045\u001b[0m, in \u001b[0;36mprod\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2927\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_prod_dispatcher)\n\u001b[1;32m   2928\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprod\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue,\n\u001b[1;32m   2929\u001b[0m          initial\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue, where\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue):\n\u001b[1;32m   2930\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2931\u001b[0m \u001b[39m    Return the product of array elements over a given axis.\u001b[39;00m\n\u001b[1;32m   2932\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3043\u001b[0m \u001b[39m    10\u001b[39;00m\n\u001b[1;32m   3044\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3045\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapreduction(a, np\u001b[39m.\u001b[39;49mmultiply, \u001b[39m'\u001b[39;49m\u001b[39mprod\u001b[39;49m\u001b[39m'\u001b[39;49m, axis, dtype, out,\n\u001b[1;32m   3046\u001b[0m                           keepdims\u001b[39m=\u001b[39;49mkeepdims, initial\u001b[39m=\u001b[39;49minitial, where\u001b[39m=\u001b[39;49mwhere)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:75\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(obj) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m mu\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m     74\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 75\u001b[0m         reduction \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(obj, method)\n\u001b[1;32m     76\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m         \u001b[39mpass\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(env_name, render_mode='rgb_array')   # ] simulation \n",
    "list = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1]\n",
    "container = []\n",
    "alpha = 0\n",
    "\n",
    "for iteration in tqdm(range(20)):\n",
    "    alpha +=0.05\n",
    "    print(alpha)\n",
    "    for gamma in list:\n",
    "        for epsilon in list: \n",
    "            SARSA_agent = SARSA_learning(intervals = intervals, n_split = n_split, env = env, epsilon=epsilon, alpha = alpha, gamma=gamma)\n",
    "            SARSA_agent.run()\n",
    "            x = np.arange(len(SARSA_agent.timestamps))\n",
    "            accumul = 0\n",
    "            mobile_avg = []\n",
    "            counter = 0\n",
    "            for elem in SARSA_agent.timestamps: \n",
    "                counter += 1\n",
    "                accumul = accumul + (1/counter)*(elem-accumul) \n",
    "                mobile_avg.append(accumul)\n",
    "            container.append(mobile_avg[len(mobile_avg)-1])\n",
    "\n",
    "\n",
    "# x = np.arange(len(SARSA_agent.timestamps))\n",
    "# accumul = 0\n",
    "# mobile_avg = []\n",
    "# counter = 0\n",
    "# for elem in SARSA_agent.timestamps: \n",
    "#     counter += 1\n",
    "#     accumul = accumul + (1/counter)*(elem-accumul) \n",
    "#     mobile_avg.append(accumul)\n",
    "    \n",
    "# plt.plot(x,mobile_avg, label='eps = 0.95, gamma = 1')  # replace accordingly with the parameters used in the algorithm\n",
    "# plt.axhline(195.0, color='gray', label='env threshold = 195.0', linestyle='--')\n",
    "# plt.xlabel('n° episodes')\n",
    "# plt.ylabel('avg returns') \n",
    "# plt.title('learning curve')\n",
    "# plt.legend()\n",
    "# plt.plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_name, render_mode='human')       # for rendering\n",
    "\n",
    "# let's experience with the environment  \n",
    "observation, info = env.reset()\n",
    "curr_state = state_projection(observation, SARSA_agent.intervals)\n",
    "for _ in range(10000):\n",
    "   \n",
    "   action = SARSA_agent.action_choice(curr_state) \n",
    "   new_state, reward, terminated, truncated, info = env.step(action)\n",
    "   curr_state = state_projection(new_state, SARSA_agent.intervals)\n",
    "\n",
    "   if terminated or truncated:\n",
    "      observation, info = env.reset() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
