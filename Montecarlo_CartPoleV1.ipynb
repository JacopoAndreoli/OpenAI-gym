{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CartPole_env import CartPole_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "CartPole = CartPole_v1(PLOT_DEBUG = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MC_learning():\n",
    "    \n",
    "    '''\n",
    "    class that construct the Montecarlo approach to the CartPole problem. The methods inside the class \n",
    "    already have a sel-explanatory name of their functionality \n",
    "    '''\n",
    "    def __init__(self, intervals, n_split, env, epsilon = 0.98, gamma = 0.9, alpha = 0.8):\n",
    "        \n",
    "        self.intervals = intervals\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha \n",
    "        self.env = env\n",
    "        self.timestamps = []\n",
    "        Q_height = 2 # since there are only two possible action \n",
    "        Q_width = (n_split[0]+2)*(n_split[1]+2)*(n_split[2]+2)*(n_split[3]+2)\n",
    "        self.Q_table = np.array([np.random.choice([0, 1], size=Q_width, p=[.5, .5]), np.random.choice([0, 1], size=Q_width, p=[.5, .5])])        \n",
    "        self.Counter_table = np.zeros_like(self.Q_table) \n",
    "        # the counter table is used since the update of the Q_table will have a NOT constant 'alpha' that is alpha = 1/n.\n",
    "        # However, it is not always the best choice using a value that depends on the number of observed returns; it is not the case \n",
    "        # since the distribution that governing the physic process does not change over time \n",
    "        \n",
    "    def action_choice(self, state):\n",
    "        \n",
    "        rand_value = np.random.randint(0, 100+1)/100 \n",
    "        if(rand_value > self.epsilon):\n",
    "            action = int(np.random.choice([0, 1], size=1, p=[.5, .5]))\n",
    "        else:\n",
    "            action = np.argmax(self.Q_table[:, state]) \n",
    "        return action  \n",
    "    \n",
    "    def play_an_episode(self):\n",
    "        \n",
    "        obs_act_rew = []\n",
    "        curr_state, info = self.env.reset()\n",
    "        curr_state = state_projection(curr_state, self.intervals)\n",
    "        index = 0\n",
    "        episode_reward = []\n",
    "        while(True):\n",
    "            index +=1\n",
    "            action = self.action_choice(curr_state)\n",
    "            new_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "            episode_reward.append(reward)\n",
    "            obs_act_rew.append([curr_state, action])\n",
    "            curr_state = state_projection(new_state, self.intervals)\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                #print(\"episode last for {} timestemps\".format(index))\n",
    "                self.timestamps.append(index)\n",
    "                for k in range(0,len(episode_reward),1):\n",
    "                    if k == 0:\n",
    "                        returns = episode_reward\n",
    "                    else:\n",
    "                        returns = list(reversed(episode_reward))[:-k]\n",
    "                    obs_act_rew[k].append(math.fsum(returns))\n",
    "                break\n",
    "        \n",
    "        return obs_act_rew    \n",
    "\n",
    "    def update_Q_table(self, list_of_returns):\n",
    "        \n",
    "        G = 0\n",
    "        for elem in list_of_returns[::-1]:\n",
    "            state, action, reward = elem[0], elem[1], elem[2] \n",
    "            G = self.gamma*G + reward\n",
    "            self.Counter_table[action][state] += 1\n",
    "            self.Q_table[action][state] = self.Q_table[action][state] + (1/self.Counter_table[action][state])*(G-self.Q_table[action][state])\n",
    "            #self.alpha*(G-self.Q_table[action][state])\n",
    "            \n",
    "            \n",
    "    def run(self, n_episodes = 5000): \n",
    "        for _ in tqdm(range(n_episodes)):\n",
    "            list_of_returns = self.play_an_episode()\n",
    "            self.update_Q_table(list_of_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "initialization and run of the algorithm; here it is important to approapriately set the epsilon and gamma parameter \n",
    "so as to obtain the best efficiency, namely learn the optimal policy in the fastest way\n",
    "'''\n",
    "MC_agent = MC_learning(intervals = intervals, n_split = n_split, env = env, epsilon=.95, gamma=1, alpha = 0.99)\n",
    "MC_agent.run(5000)\n",
    "x = np.arange(len(MC_agent.timestamps))\n",
    "accumul = 0\n",
    "mobile_avg = []\n",
    "counter = 0\n",
    "for elem in MC_agent.timestamps: \n",
    "    counter += 1\n",
    "    accumul = accumul + (1/counter)*(elem-accumul) \n",
    "    mobile_avg.append(accumul)\n",
    "    \n",
    "plt.plot(x,mobile_avg, label='eps = 0.95, gamma = 1')  # replace accordingly with the parameters used in the algorithm\n",
    "plt.axhline(195.0, color='gray', label='env threshold = 195.0', linestyle='--')\n",
    "plt.xlabel('nÂ° episodes')\n",
    "plt.ylabel('avg returns') \n",
    "plt.title('learning curve')\n",
    "plt.legend()\n",
    "plt.plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In this cell, using the correct render option, it is possible to visualize thorugh real-time simulation the\n",
    "final result associated to the policy learnt by the agent\n",
    "'''\n",
    "#env = gym.make(env_name, render_mode='rgb_array')   # for simulation\n",
    "env = gym.make(env_name, render_mode='human')       # for rendering\n",
    "\n",
    "# let's experience with the environment  \n",
    "observation, info = env.reset()\n",
    "curr_state = state_projection(observation, MC_agent.intervals)\n",
    "for _ in range(10000):\n",
    "   action = MC_agent.action_choice(curr_state) \n",
    "   new_state, reward, terminated, truncated, info = env.step(action)\n",
    "   curr_state = state_projection(new_state, MC_agent.intervals)\n",
    "\n",
    "   if terminated or truncated:\n",
    "      observation, info = env.reset() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
